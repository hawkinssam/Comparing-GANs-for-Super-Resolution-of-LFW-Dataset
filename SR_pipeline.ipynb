{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline for ME740 Project done by Sam Hawkins\n",
        "## I recommend using Google Colaboratory to run this notebook, as there are many different downloads of packages and requirement files. This notebook can be extremely computationally expensive.\n",
        "## To run the code and view your own results, simply run the cells in order. If you need to reduce computational complexity, reduce the \"resize\" option in the LFW dataset import to a lower fraction\n",
        "### Open-Source repositories used in this notebook directly: https://github.com/xinntao/BasicSR, https://github.com/serengil/deepface, https://github.com/TencentARC/GFPGAN "
      ],
      "metadata": {
        "id": "AYU28P91nZZv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dad9ZFV6Qp5p"
      },
      "source": [
        "# First Step: ESRGAN Inference -> Super Resolution of LFW dataset using ESRGAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY1Zbo3uUkXg"
      },
      "source": [
        "# Preparations\n",
        "Before start, make sure that you choose\n",
        "* Runtime Type = Python 3\n",
        "* Hardware Accelerator = GPU\n",
        "\n",
        "in the **Runtime** menu -> **Change runtime type**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gntSIyaNAz_L"
      },
      "source": [
        "## Git clone BasicSR repo which contains ESRGAN model and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v4E3BgHUYWE"
      },
      "outputs": [],
      "source": [
        "!rm -rf BasicSR\n",
        "!git clone https://github.com/xinntao/BasicSR.git\n",
        "%cd BasicSR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_UO_IB7VBE7"
      },
      "source": [
        "## Set up the enviroment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHRK1FtlVFzf"
      },
      "outputs": [],
      "source": [
        "# Install pytorch\n",
        "!pip install torch torchvision\n",
        "\n",
        "# Check torch and cuda versions\n",
        "import torch\n",
        "print('Torch Version: ', torch.__version__)\n",
        "print('CUDA Version: ', torch.version.cuda)\n",
        "print('CUDNN Version: ', torch.backends.cudnn.version())\n",
        "print('CUDA Available:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqL-o4RlIQAL"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "# Install BasicSR without cuda extentions\n",
        "!python setup.py develop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLzXEFrdTvwW"
      },
      "source": [
        "## Download pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qksBZXznTy5F"
      },
      "outputs": [],
      "source": [
        "!python scripts/download_pretrained_models.py ESRGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZlxwejEuNRN"
      },
      "source": [
        "## Set proper folders in directory to place all necessary images inside"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHcDCFnkZ0so"
      },
      "source": [
        "### Create folders in directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7imoNF6DvjPw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "os.makedirs('results/ESRGAN', exist_ok=True)\n",
        "\n",
        "upload_folder = 'datasets/upload'\n",
        "result_folder = 'results/ESRGAN'\n",
        "compare_folder = 'results/interpolation'\n",
        "LR_folder = 'datasets/LR'\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "if os.path.isdir(compare_folder):\n",
        "    shutil.rmtree(compare_folder)\n",
        "if os.path.isdir(LR_folder):\n",
        "    shutil.rmtree(LR_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)\n",
        "os.mkdir(compare_folder)\n",
        "os.mkdir(LR_folder)\n",
        "\n",
        "\"\"\"\n",
        "# upload images\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  dst_path = os.path.join(upload_folder, filename)\n",
        "  print(f'move {filename} to {dst_path}')\n",
        "  shutil.move(filename, dst_path)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUFn1oyVQHqw"
      },
      "source": [
        "# Testing Phase 1 - Perform Super Resolution on LFW pairs, compare recognition rates for original, Bicubic Interpolation, and for ESRGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get requirements and packages"
      ],
      "metadata": {
        "id": "-LZBfZktrXzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_IQFYeC-Ls2"
      },
      "outputs": [],
      "source": [
        "#Face recognition model install\n",
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoecvPy_7Rma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ad34de-575c-4c7c-ec4e-0d3175719b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory  /root /.deepface created\n",
            "Directory  /root /.deepface/weights created\n"
          ]
        }
      ],
      "source": [
        "from deepface import DeepFace\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viR0V5MN5q0i"
      },
      "source": [
        "## Import LFW dataset.\n",
        "### Run as is for color images. If you want to test on Black and White images, set color to False. If doing Black and White testing, uncomment the commented lines the have /BW at the end and comment the corresponding color counterpart code line\n",
        "### Using the training subset for more images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0WWaAe14gNN"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_pairs\n",
        "#fetch_lfw_pairs(subset='train',data_home=None,funneled=True\n",
        "#,resize=0.5,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)\n",
        "#original img size (250,250), size with default resize and slice (62,47)\n",
        "fetch_lfw_pairs = fetch_lfw_pairs(subset = 'train', funneled=True\n",
        ", color = True, resize = .5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79CY52BNTAPT"
      },
      "source": [
        "## Define pairs\n",
        "### For 'test', there are 1000 images, first 500 have same person pairs, second 500 have different people pairs. For 'train', 2200 images, 1100 same, 1100 different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3qqR9ObTAPU"
      },
      "outputs": [],
      "source": [
        "pairs = fetch_lfw_pairs.pairs\n",
        "labels = fetch_lfw_pairs.target\n",
        "target_names = fetch_lfw_pairs.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4RUss2rThmM"
      },
      "outputs": [],
      "source": [
        "pairs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiDajKBNTF0T"
      },
      "outputs": [],
      "source": [
        "all_img = pairs.reshape(pairs.shape[0]*2,pairs.shape[2],pairs.shape[3],pairs.shape[4])\n",
        "#all_img = pairs.reshape(pairs.shape[0]*2,pairs.shape[2],pairs.shape[3])#BW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nw6KvDAV83G"
      },
      "source": [
        "### Add LR images to LR folder in directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNlIeEkSYZI_"
      },
      "outputs": [],
      "source": [
        "for i in range(len(all_img)):\n",
        "    img = all_img[i]\n",
        "    cv2.imwrite(f'datasets/LR/{i}.png',img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYr7wffmbbLJ"
      },
      "source": [
        "### SR by Bicubic Interpolation of LR images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyRGDPt1V8hv"
      },
      "outputs": [],
      "source": [
        "folder = 'datasets/LR'\n",
        "\n",
        "os.makedirs('results/interpolation', exist_ok=True)\n",
        "for idx, path in enumerate(sorted(glob.glob(os.path.join(folder, '*')))):\n",
        "    imgname = os.path.splitext(os.path.basename(path))[0]\n",
        "    #print(idx, imgname)\n",
        "    # read image\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
        "    #img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255. #BW\n",
        "\n",
        "\n",
        "    # inference\n",
        "    output = cv2.resize(img,(img.shape[1]*4,img.shape[0]*4))\n",
        "\n",
        "    # save image\n",
        "    #output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
        "    output = (output * 255.0).round().astype(np.uint8)\n",
        "\n",
        "    cv2.imwrite(f'results/interpolation/{imgname}_interp.png', output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyLWoAU1cN-j"
      },
      "source": [
        "### SR by ESRGAN of LR images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ_54p-jcadb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "\n",
        "# configuration\n",
        "model_path = 'experiments/pretrained_models/ESRGAN/ESRGAN_SRx4_DF2KOST_official-ff704c30.pth'\n",
        "\n",
        "folder = 'datasets/LR'\n",
        "device = 'cuda'\n",
        "\n",
        "device = torch.device(device)\n",
        "\n",
        "# set up model\n",
        "model = RRDBNet(\n",
        "    num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32)\n",
        "model.load_state_dict(torch.load(model_path)['params'], strict=True)\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "os.makedirs('results/ESRGAN', exist_ok=True)\n",
        "for idx, path in enumerate(sorted(glob.glob(os.path.join(folder, '*')))):\n",
        "    imgname = os.path.splitext(os.path.basename(path))[0]\n",
        "    #print(idx, imgname)\n",
        "    # read image\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
        "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]],\n",
        "                                        (2, 0, 1))).float()\n",
        "    img = img.unsqueeze(0).to(device)\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "    # save image\n",
        "    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "    output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
        "    output = (output * 255.0).round().astype(np.uint8)\n",
        "    cv2.imwrite(f'results/ESRGAN/{imgname}_ESRGAN.png', output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRUiv7S_yFOy"
      },
      "source": [
        "### Download Results (OPTIONAL) - Uncomment if you want zip files of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_fQq2R5vfOW"
      },
      "outputs": [],
      "source": [
        "# download the ESRGAN result\n",
        "\"\"\"print(f'Download {result_folder}')\n",
        "os.system(f'zip -r -j download.zip {result_folder}/*')\n",
        "files.download(\"download.zip\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F727NKCgvfOX"
      },
      "outputs": [],
      "source": [
        "# download the bicubic\n",
        "\"\"\"\n",
        "print(f'Download {compare_folder}')\n",
        "os.system(f'zip -r -j bicubic.zip {compare_folder}/*')\n",
        "files.download(\"bicubic.zip\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ZJn1_qeKhh"
      },
      "source": [
        "## Get all three sets of results back into pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3RZ7msXk-du"
      },
      "source": [
        "### Extract images from result folders into variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7KSE3AW91VO"
      },
      "outputs": [],
      "source": [
        "import natsort\n",
        "#sorted = natsort.natsorted(os.listdir(path))\n",
        "def extract_results(path,all_lr):\n",
        "\n",
        "    #images = np.zeros((all_lr.shape[0],all_lr.shape[1]*4,all_lr.shape[2]*4,all_lr.shape[3]))\n",
        "    images = []\n",
        "\n",
        "    for file in natsort.natsorted(os.listdir(path)):\n",
        "        img = os.path.join(path,file)\n",
        "        img = cv2.imread(img,cv2.IMREAD_COLOR)\n",
        "        #img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)#BW\n",
        "        #images[i] = img\n",
        "        images.append(img)\n",
        "\n",
        "    images = images[:all_lr.shape[0]] #in order to save RAM if necessary\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5uopFYeP6Qa"
      },
      "outputs": [],
      "source": [
        "interp_results = np.array(extract_results('results/interpolation',all_img))\n",
        "esrgan_results = np.array(extract_results('results/ESRGAN',all_img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpcOlOmAQ2s8"
      },
      "outputs": [],
      "source": [
        "interp_results.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i8ZlJZClGSN"
      },
      "source": [
        "# Testing Phase 2: Run Face Optimized GAN model (GFPGAN) and extract results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yxea9_7CStc"
      },
      "source": [
        "## Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdeFHd0ECStj"
      },
      "outputs": [],
      "source": [
        "# Clone GFPGAN and enter the GFPGAN folder\n",
        "%cd /content\n",
        "!rm -rf GFPGAN\n",
        "!git clone https://github.com/TencentARC/GFPGAN.git\n",
        "%cd GFPGAN\n",
        "\n",
        "# Set up the environment\n",
        "# Install basicsr - https://github.com/xinntao/BasicSR\n",
        "# We use BasicSR for both training and inference\n",
        "\n",
        "# Install facexlib - https://github.com/xinntao/facexlib\n",
        "# We use face detection and face restoration helper in the facexlib package\n",
        "!pip install facexlib\n",
        "# Install other depencencies\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install realesrgan  # used for enhancing the background (non-face) regions\n",
        "# Download the pre-trained model\n",
        "# !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models\n",
        "# Now we use the V1.3 model for the demo\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErYlff-nCStj"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6LeumoPnqmk"
      },
      "outputs": [],
      "source": [
        "os.makedirs('datasets/LR',exist_ok=True)\n",
        "for i in range(len(all_img)):\n",
        "    img = all_img[i]\n",
        "    cv2.imwrite(f'datasets/LR/{i}.png',img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJosYlbRCStj"
      },
      "outputs": [],
      "source": [
        "# Now we use the GFPGAN to restore the above low-quality images\n",
        "# We use [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for enhancing the background (non-face) regions\n",
        "# You can find the different models in https://github.com/TencentARC/GFPGAN#european_castle-model-zoo\n",
        "\n",
        "!rm -rf results\n",
        "!python inference_gfpgan.py -i datasets/LR -o results -v 1.3 -s 4 --bg_upsampler realesrgan --suffix gfpgan --ext png\n",
        "\n",
        "# Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n",
        "# \n",
        "#  -h                   show this help\n",
        "#  -i input             Input image or folder. Default: inputs/whole_imgs\n",
        "#  -o output            Output folder. Default: results\n",
        "#  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3\n",
        "#  -s upscale           The final upsampling scale of the image. Default: 2\n",
        "#  -bg_upsampler        background upsampler. Default: realesrgan\n",
        "#  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400\n",
        "#  -suffix              Suffix of the restored faces\n",
        "#  -only_center_face    Only restore the center face\n",
        "#  -aligned             Input are aligned faces\n",
        "#  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n",
        "\n",
        "!ls results/cmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHHJJdoulDeC"
      },
      "outputs": [],
      "source": [
        "gfpgan_results = np.array(extract_results('results/restored_imgs',all_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NhGSaw4Tdjn"
      },
      "source": [
        "## Check to make sure the orders align, and visualize some example results\n",
        "### Functions to visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U95_DskcoS2U"
      },
      "outputs": [],
      "source": [
        "def plot_results(orig,interp,esrgan,gfpgan):\n",
        "    fig = plt.figure(figsize=(25, 10))\n",
        "    ax1 = fig.add_subplot(1, 4, 1) \n",
        "    plt.title('Input image', fontsize=16)\n",
        "    ax1.axis('off')\n",
        "    ax2 = fig.add_subplot(1, 4, 2)\n",
        "    plt.title('Bicubic Interpolation', fontsize=16)\n",
        "    ax2.axis('off')\n",
        "    ax3 = fig.add_subplot(1, 4, 3)\n",
        "    plt.title('ESRGAN output', fontsize=16)\n",
        "    ax3.axis('off')\n",
        "    ax4 = fig.add_subplot(1, 4, 4)\n",
        "    plt.title('GFPGAN output', fontsize=16)\n",
        "    ax4.axis('off')\n",
        "    ax1.imshow(orig/255)\n",
        "    ax2.imshow(interp)\n",
        "    ax3.imshow(esrgan)\n",
        "    ax4.imshow(gfpgan)\n",
        "    \n",
        "    #ax1.imshow(orig,cmap='gray',vmin=0,vmax=255)\n",
        "    #ax2.imshow(interp,cmap='gray',vmin=0,vmax=255)\n",
        "    #ax3.imshow(esrgan,cmap='gray',vmin=0,vmax=255)\n",
        "    #ax4.imshow(gfpgan,cmap='gray',vmin=0,vmax=255)\n",
        "    \n",
        "\n",
        "def plot_rand(num,orig,interp,esrgan,gfpgan):\n",
        "    for i in range(num):\n",
        "        randy = np.random.randint(len(interp))\n",
        "        plot_results(orig[randy],interp[randy],esrgan[randy],gfpgan[randy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONJhTdU_qxe1"
      },
      "source": [
        "## Visualize some images\n",
        "### To change number of images to view, change number to the amount of result comparisons you want to see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8FgTRpfpK-7"
      },
      "outputs": [],
      "source": [
        "plot_rand(40,all_img,interp_results,esrgan_results,gfpgan_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IlMlDsTtVAr"
      },
      "source": [
        "### Transform all sets back into pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9wP_ncZv4fe"
      },
      "outputs": [],
      "source": [
        "def reshape_pairs(images):\n",
        "    #Note that there needs to be even number in original array, as we need pairs\n",
        "    reshaped = images.reshape(int(images.shape[0]/2), 2, images.shape[1], images.shape[2], images.shape[3])\n",
        "    #reshaped = images.reshape(int(images.shape[0]/2), 2, images.shape[1], images.shape[2])#BW\n",
        "\n",
        "    return reshaped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0mOnc1Owvqk"
      },
      "outputs": [],
      "source": [
        "esrgan_pairs = reshape_pairs(esrgan_results)\n",
        "interp_pairs = reshape_pairs(interp_results)\n",
        "gfpgan_pairs = reshape_pairs(gfpgan_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1cbiVHt6GRC"
      },
      "source": [
        "## (OPTIONAL) Plot the pairs, ensuring the same people are in pairs from different image sets\n",
        "### This shows the pairs and the ground truth of whether or not it's the same person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U3pNd6f5en4"
      },
      "outputs": [],
      "source": [
        "def plot_pairs(num, pairs, labels):\n",
        "\n",
        "    for i in range(0, num):\n",
        "      pair = pairs[i]\n",
        "      img1 = pair[0]\n",
        "      img2 = pair[1]\n",
        "    \n",
        "      fig = plt.figure()\n",
        "    \n",
        "      ax1 = fig.add_subplot(1,3,1)\n",
        "      plt.imshow(img1/255)\n",
        "    \n",
        "      ax2 = fig.add_subplot(1,3,2)\n",
        "      plt.imshow(img2/255)\n",
        "    \n",
        "      ax3 = fig.add_subplot(1,3,3)\n",
        "      plt.text(0, 0.50, target_names[labels[i]])\n",
        "    \n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dG51g-0xwsS"
      },
      "outputs": [],
      "source": [
        "\"\"\"plot_pairs(5,esrgan_pairs, labels)\n",
        "plot_pairs(5,interp_pairs, labels)\n",
        "plot_pairs(5,pairs, labels)\n",
        "plot_pairs(5, gfpgan_pairs, labels)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTTYBxFS6XtF"
      },
      "source": [
        "# Final Step: Compare Facial Recognition accuracy on the three sets\n",
        "## Import Facenet model from deepface framework\n",
        "### Run recognition to tell whether or not the pairs have the same or different people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxZIrYpCBll9"
      },
      "outputs": [],
      "source": [
        "def recognition_predictions(img_pairs, model_name = 'Facenet'):\n",
        "\n",
        "    #model_name options: (string) VGG-Face, Facenet, OpenFace, DeepFace, DeepID, Dlib, ArcFace or Ensemble\n",
        "    #img_pairs is images we are comparing: dimensions(n-pairs,2, img_h,img_w,#channels)\n",
        "    model = DeepFace.build_model(model_name)\n",
        "    predictions = []\n",
        "\n",
        "    #If Black and white\n",
        "    #img_pairs = np.stack((img_pairs,)*3, axis=-1)  \n",
        "\n",
        "    for idx in tqdm(range(0,img_pairs.shape[0])):\n",
        "\n",
        "        pair = img_pairs[idx]\n",
        "        img1 = pair[0]\n",
        "        img2 = pair[1]\n",
        "\n",
        "        #actual = labels[idx]\n",
        "        obj = DeepFace.verify(img1, img2, model_name = model_name,\n",
        "                              model=model, enforce_detection = False,\n",
        "                              detector_backend = 'opencv')\n",
        "        \n",
        "        prediction = 1 if obj[\"verified\"] == True else 0\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract results into variables - This step takes the longest"
      ],
      "metadata": {
        "id": "uvF_h7irqdm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvK-oij3Mcga"
      },
      "outputs": [],
      "source": [
        "orig_recog = recognition_predictions(pairs, model_name='Facenet')\n",
        "interp_recog = recognition_predictions(interp_pairs, model_name='Facenet')\n",
        "esrgan_recog = recognition_predictions(esrgan_pairs, model_name='Facenet')\n",
        "gfpgan_recog = recognition_predictions(gfpgan_pairs, model_name='Facenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjrV01bu6nEe"
      },
      "source": [
        "## Determine results of recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UVRjU545kRA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def compute_performance(actuals,predictions):\n",
        "\n",
        "    #Accuracy, precision, recall, F1\n",
        "    accuracy = 100*accuracy_score(actuals, predictions)\n",
        "    precision = 100*precision_score(actuals, predictions)\n",
        "    recall = 100*recall_score(actuals, predictions)\n",
        "    f1 = 100*f1_score(actuals, predictions)\n",
        "    print(\"Accuracy = \",accuracy,\"%\")\n",
        "    print(\"Precision = \",precision,\"%\")\n",
        "    print(\"Recall = \",recall,\"%\")\n",
        "    print(\"F1 Score = \",f1,\"%\")\n",
        "\n",
        "    #Confusion matrix\n",
        "    cm = confusion_matrix(actuals, predictions)\n",
        "    print(\"\\nConfusion Matrix:\\n\",cm)\n",
        "    \n",
        "    #Numbers of true/false positives/negatives from CM\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    print(\"\\nTrue Negative = \",tn)\n",
        "    print(\"False Positive = \", fp)\n",
        "    print(\"False Negative = \",fn)\n",
        "    print(\"True Positive = \",tp ,'\\n')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show results"
      ],
      "metadata": {
        "id": "kVzn8UmyqobA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f9N9MZRImcl"
      },
      "outputs": [],
      "source": [
        "compute_performance(labels, orig_recog)\n",
        "compute_performance(labels, interp_recog)\n",
        "compute_performance(labels, esrgan_recog)\n",
        "compute_performance(labels, gfpgan_recog)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "t1cbiVHt6GRC"
      ],
      "name": "SamHawkins_ME740_SRpipeline",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}